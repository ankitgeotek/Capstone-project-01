{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change as per your requirement - city name\n",
    "# Match with 99acers site like for chandighars flats data site is : https://www.99acres.com/flats-in-chandigarh-ffid\n",
    "# Taking value of city as 'chandigarh'\n",
    "City = 'gurgaon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'authority': 'www.99acres.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': f'https://www.99acres.com/flats-in-{City}-ffid-page',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: A:\\Work Docs\\Data Analyst work\\Campus X\\10 Streamlit\\Capstone\\Required Files\\Data\n",
      "Directory already exists: A:\\Work Docs\\Data Analyst work\\Campus X\\10 Streamlit\\Capstone\\Required Files\\Data/gurgaon\n",
      "Directory already exists: A:\\Work Docs\\Data Analyst work\\Campus X\\10 Streamlit\\Capstone\\Required Files\\Data/gurgaon/Flats\n",
      "Directory already exists: A:\\Work Docs\\Data Analyst work\\Campus X\\10 Streamlit\\Capstone\\Required Files\\Data/gurgaon/Societies\n",
      "Directory already exists: A:\\Work Docs\\Data Analyst work\\Campus X\\10 Streamlit\\Capstone\\Required Files\\Data/gurgaon/Residential\n",
      "Directory already exists: A:\\Work Docs\\Data Analyst work\\Campus X\\10 Streamlit\\Capstone\\Required Files\\Data/gurgaon/Independent House\n"
     ]
    }
   ],
   "source": [
    "# If folder structures are in already created no need to run it.\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the path to your project directory\n",
    "project_dir = r'A:\\Work Docs\\Data Analyst work\\Campus X\\10 Streamlit\\Capstone\\Required Files'\n",
    "\n",
    "# Define the subdirectories\n",
    "subdirectories = ['Data', f'Data/{City}', f'Data/{City}/Flats', f'Data/{City}/Societies', f'Data/{City}/Residential', f'Data/{City}/Independent House']\n",
    "\n",
    "# Create the directory structure\n",
    "for subdir in subdirectories:\n",
    "    dir_path = os.path.join(project_dir, subdir)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(f\"Created directory: {dir_path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {dir_path}\")\n",
    "\n",
    "# Now, your directory structure is created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put start page number and end page number.\n",
    "\n",
    "# Page number to start extraction data\n",
    "start = int(input(\"Enter page number where you got error in last run.\\nEnter page number to start from:\")) # Starting Page\n",
    "\n",
    "# End Page number- you can change is for start i am taking 10pages at a time,\n",
    "# as IPs are gettig block after some time\n",
    "end = start+10\n",
    "\n",
    "pageNumber = start\n",
    "req=0\n",
    "\n",
    "flats = pd.DataFrame()\n",
    "\n",
    "try :\n",
    "    while pageNumber < end:\n",
    "        i=1\n",
    "        url = f'https://www.99acres.com/flats-in-{City}-ffid-page-{pageNumber}'\n",
    "        page = requests.get(url, headers=headers)\n",
    "        pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "        req+=1\n",
    "        for soup in pageSoup.select_one('div[data-label=\"SEARCH\"]').select('section[data-hydration-on-demand=\"true\"]'):\n",
    "\n",
    "        # Extract property name and property sub-name\n",
    "            try:\n",
    "                property_name = soup.select_one('a.srpTuple__propertyName').text.strip()\n",
    "                # Extract link\n",
    "                link = soup.select_one('a.srpTuple__propertyName')['href']\n",
    "                society = soup.select_one('#srp_tuple_society_heading').text.strip()\n",
    "            except:\n",
    "                continue\n",
    "            # Detail Page\n",
    "            page = requests.get(link, headers=headers)\n",
    "            dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "            req += 1\n",
    "            try:\n",
    "                #price Range\n",
    "                price = dpageSoup.select_one('#pdPrice2').text.strip()\n",
    "            except:\n",
    "                price = ''\n",
    "\n",
    "            # Area\n",
    "            try:\n",
    "                area = soup.select_one('#srp_tuple_price_per_unit_area').text.strip()\n",
    "            except:\n",
    "                area =''\n",
    "            # Area with Type\n",
    "            try:\n",
    "                areaWithType = dpageSoup.select_one('#factArea').text.strip()\n",
    "            except:\n",
    "                areaWithType = ''\n",
    "\n",
    "\n",
    "            # Configuration\n",
    "            try:\n",
    "                bedRoom = dpageSoup.select_one('#bedRoomNum').text.strip()\n",
    "            except:\n",
    "                bedRoom = ''\n",
    "            try:\n",
    "                bathroom = dpageSoup.select_one('#bathroomNum').text.strip()\n",
    "            except:\n",
    "                bathroom = ''\n",
    "            try:\n",
    "                balcony = dpageSoup.select_one('#balconyNum').text.strip()\n",
    "            except:\n",
    "                balcony = ''\n",
    "\n",
    "            try:\n",
    "                additionalRoom = dpageSoup.select_one('#additionalRooms').text.strip()\n",
    "            except:\n",
    "                additionalRoom = ''\n",
    "\n",
    "\n",
    "            # Address\n",
    "\n",
    "            try:\n",
    "                address = dpageSoup.select_one('#address').text.strip()\n",
    "            except:\n",
    "                address = ''\n",
    "            # Floor Number\n",
    "            try:\n",
    "                floorNum = dpageSoup.select_one('#floorNumLabel').text.strip()\n",
    "            except:\n",
    "                floorNum = ''\n",
    "\n",
    "            try:\n",
    "                facing = dpageSoup.select_one('#facingLabel').text.strip()\n",
    "            except:\n",
    "                facing = ''\n",
    "\n",
    "            try:\n",
    "                agePossession = dpageSoup.select_one('#agePossessionLbl').text.strip()\n",
    "            except:\n",
    "                agePossession = ''\n",
    "\n",
    "            # Nearby Locations\n",
    "\n",
    "            try:\n",
    "                nearbyLocations = [i.text.strip() for i in dpageSoup.select_one('div.NearByLocation__tagWrap').select('span.NearByLocation__infoText')]\n",
    "            except:\n",
    "                nearbyLocations = ''\n",
    "\n",
    "            # Descriptions\n",
    "            try:\n",
    "                description = dpageSoup.select_one('#description').text.strip()\n",
    "            except:\n",
    "                description = ''\n",
    "\n",
    "            # Furnish Details\n",
    "            try:\n",
    "                furnishDetails = [i.text.strip() for i in dpageSoup.select_one('#FurnishDetails').select('li')]\n",
    "            except:\n",
    "                furnishDetails = ''\n",
    "\n",
    "            # Features\n",
    "            if furnishDetails:\n",
    "                try:\n",
    "                    features = [i.text.strip() for i in dpageSoup.select('#features')[1].select('li')]\n",
    "                except:\n",
    "                    features = ''\n",
    "            else:\n",
    "                try:\n",
    "                    features = [i.text.strip() for i in dpageSoup.select('#features')[0].select('li')]\n",
    "                except:\n",
    "                    features = ''\n",
    "\n",
    "\n",
    "\n",
    "            # Rating by Features\n",
    "            try:\n",
    "                rating = [i.text for i in dpageSoup.select_one('div.review__rightSide>div>ul>li>div').select('div.ratingByFeature__circleWrap')]\n",
    "            except:\n",
    "                rating = ''\n",
    "            # print(top_f)\n",
    "\n",
    "            try:\n",
    "                # Property ID\n",
    "                property_id = dpageSoup.select_one('#Prop_Id').text.strip()\n",
    "            except:\n",
    "                property_id = ''\n",
    "\n",
    "            # Create a dictionary with the given variables\n",
    "            property_data = {\n",
    "            'property_name': property_name,\n",
    "            'link': link,\n",
    "            'society': society,\n",
    "            'price': price,\n",
    "            'area': area,\n",
    "            'areaWithType': areaWithType,\n",
    "            'bedRoom': bedRoom,\n",
    "            'bathroom': bathroom,\n",
    "            'balcony': balcony,\n",
    "            'additionalRoom': additionalRoom,\n",
    "            'address': address,\n",
    "            'floorNum': floorNum,\n",
    "            'facing': facing,\n",
    "            'agePossession': agePossession,\n",
    "            'nearbyLocations': nearbyLocations,\n",
    "            'description': description,\n",
    "            'furnishDetails': furnishDetails,\n",
    "            'features': features,\n",
    "            'rating': rating,\n",
    "            'property_id': property_id\n",
    "        }\n",
    "\n",
    "\n",
    "            temp_df = pd.DataFrame.from_records([property_data])\n",
    "            # print(temp_df)\n",
    "            flats = pd.concat([flats, temp_df], ignore_index=True)\n",
    "            i += 1\n",
    "            # if os.path.isfile(csv_file):\n",
    "            # # Append DataFrame to the existing file without header\n",
    "            #     temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "            # else:\n",
    "            #     # Write DataFrame to the file with header\n",
    "            #     temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
    "\n",
    "            if req % 4==0:\n",
    "                time.sleep(10)\n",
    "            if req % 15 == 0:\n",
    "                time.sleep(50)\n",
    "        print(f'{pageNumber} -> {i}')\n",
    "        pageNumber += 1\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(e)\n",
    "    print(\"----------------\")\n",
    "    print(\"\"\"Your IP might have blocked. Delete Runitme and reconnect again with updating start page number.\\n\n",
    "            You would see in output above like 1 -> 15\\ and so 1 is page number and 15 is data items extracted.\"\"\")\n",
    "    csv_file_path = f\"/content/drive/MyDrive/DSMP/Case Studies/Real estate/Data/chandigarh/Flats/flats_{City}_data-page-{start}-{pageNumber-1}.csv\"\n",
    "\n",
    "    # This file will be new every time if start page will chnage, but still taking here mode as append\n",
    "    if os.path.isfile(csv_file_path):\n",
    "    # Append DataFrame to the existing file without header\n",
    "        flats.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write DataFrame to the file with header - first time write\n",
    "        flats.to_csv(csv_file_path, mode='a', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
